{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"unsloth/Meta-Llama-3.1-8B\", \n",
    "    max_seq_length= max_seq_length, \n",
    "    dtype= dtype, \n",
    "    load_in_4bit= load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r= 16, \n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha= 16, \n",
    "    lora_dropout= 0, \n",
    "    bias= \"none\", \n",
    "    use_gradient_checkpointing= \"unsloth\", \n",
    "    random_state= 3407, \n",
    "    use_rslora= False,\n",
    "    loftq_config= None,\n",
    ")\n",
    "#lora adapter, we set the weights as W + AB, where W is the original weights and A and B are new vectors we add. \n",
    "#W stays the same from trained model, but A and B are adjusted in training. Since W is a x by y matrix, then A is a x by 1 and B is a 1 by x vector.\n",
    "#This is significantly less weight adjusting compared to retraining the model, making it much faster\n",
    "#-> only really trainined a tenth of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('discord_messages.csv')\n",
    "df.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "print(EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def formatting_prompt(examples):\n",
    "  input = examples['input']\n",
    "  output = examples['output']\n",
    "  texts = []\n",
    "  for input, output in zip(input, output):\n",
    "    text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return { \"text\": texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model= model, \n",
    "    tokenizer= tokenizer, \n",
    "    train_dataset= dataset, \n",
    "    dataset_text_field= 'text', \n",
    "    max_seq_length= max_seq_length, \n",
    "    dataset_num_proc= 2, \n",
    "    packing= False,\n",
    "    args= TrainingArguments(\n",
    "        per_device_train_batch_size= 2, \n",
    "        gradient_accumulation_steps= 4, \n",
    "        warmup_steps= 5, \n",
    "        max_steps= 60, \n",
    "        learning_rate= 2e-4, \n",
    "        fp16= not is_bfloat16_supported(), \n",
    "        bf16= is_bfloat16_supported(), \n",
    "        logging_steps= 1, \n",
    "        optim= 'adamw_8bit', \n",
    "        weight_decay= 0.01, \n",
    "        lr_scheduler_type= 'linear',\n",
    "        seed= 3407, \n",
    "        output_dir= 'outputs', \n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf('model', tokenizer, quantization_method='f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "cd model\n",
    "ls\n",
    "cat <<EOF > Modelfile\n",
    "FROM ./unsloth.F16.gguf\n",
    "\n",
    "SYSTEM You are a chatbot that responds to the input with the appropriate output.\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "ollama serve\n",
    "ollama create llama3chat -f ./Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
