{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Wikipedia article setup\n",
    "webpage = \"Python (programming language)\" # wikipedia article title\n",
    "load_dotenv()\n",
    "\n",
    "def fetchWiki(term, wiki_lang=\"en\"):\n",
    "    wiki = wikipediaapi.Wikipedia(user_agent=os.getenv('wikipedia_useragent'), language=wiki_lang)\n",
    "    page = wiki.page(term)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "        result = page.section_by_title(\"Plot\") or page.section_by_title(\"Synopsis\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"{term} could not be found\")\n",
    "        return \"\"\n",
    "\n",
    "with open('input.txt', 'r') as f, open(\"output.txt\", \"w\") as w:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        w.write(f\"{fetchWiki(line)}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 merging (101, 32) to 256\n",
      "30 merging (97, 32) to 286\n",
      "60 merging (46, 10) to 316\n",
      "90 merging (97, 103) to 346\n",
      "120 merging (119, 268) to 376\n",
      "150 merging (111, 99) to 406\n",
      "180 merging (270, 108) to 436\n",
      "210 merging (115, 99) to 466\n",
      "240 merging (102, 105) to 496\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "vocab_size = 500\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "merge_interval = 30\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    if (i % merge_interval == 0):\n",
    "        print(f\"{i} merging {pair} to {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "with open('mergedValues.txt', 'w') as f:\n",
    "    for i in range(vocab_size):\n",
    "        f.write(f\"[{vocab[i].decode(\"utf-8\", errors=\"replace\")}] = {i}\\n\")\n",
    "\n",
    "with open('tokened.txt', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open('merges.txt', 'wb') as f:\n",
    "    pickle.dump(merges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
