{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Wikipedia article setup\n",
    "webpage = \"Python (programming language)\" # wikipedia article title\n",
    "load_dotenv()\n",
    "\n",
    "def fetchWiki(term, wiki_lang=\"en\"):\n",
    "    wiki = wikipediaapi.Wikipedia(user_agent=os.getenv('wikipedia_useragent'), language=wiki_lang)\n",
    "    page = wiki.page(term)\n",
    "    if page.exists():\n",
    "        return page.summary, page.text\n",
    "    else:\n",
    "        print(f\"{term} could not be found\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "with open('input.txt', 'r') as f, open(\"output.txt\", \"w\") as w:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        summary, text = fetchWiki(line)\n",
    "        w.write(f\"{summary}\\n\\n{text}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Theory: Iterate through string some times and get pairs of characters, the most common pair is replaced with some symbol\n",
    "\n",
    "aaabdaaabac -> ZabdZabac -> ZYdZYac -> XdXac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192808\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 merging (101, 32) to 256\n",
      "30 merging (281, 32) to 286\n",
      "60 merging (115, 117) to 316\n",
      "90 merging (274, 32) to 346\n",
      "120 merging (363, 32) to 376\n",
      "150 merging (110, 111) to 406\n",
      "180 merging (299, 115) to 436\n",
      "210 merging (111, 98) to 466\n",
      "240 merging (109, 307) to 496\n",
      "270 merging (293, 265) to 526\n",
      "300 merging (275, 382) to 556\n",
      "330 merging (374, 49) to 586\n",
      "360 merging (440, 493) to 616\n",
      "390 merging (119, 581) to 646\n",
      "420 merging (273, 471) to 676\n",
      "450 merging (273, 108) to 706\n",
      "480 merging (265, 276) to 736\n",
      "510 merging (403, 32) to 766\n",
      "540 merging (117, 279) to 796\n",
      "570 merging (326, 378) to 826\n",
      "600 merging (10, 448) to 856\n",
      "630 merging (437, 289) to 886\n",
      "660 merging (257, 287) to 916\n",
      "690 merging (334, 104) to 946\n",
      "720 merging (315, 364) to 976\n",
      "66475\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "vocab_size = 1000\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "merge_interval = 30\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    if (i % merge_interval == 0):\n",
    "        print(f\"{i} merging {pair} to {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.90\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens) / len(ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        if len(stats) == 0:\n",
    "            break\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'K'\n",
      "'im'\n"
     ]
    }
   ],
   "source": [
    "for i in encode(\"Kim\"):\n",
    "    print(f\"'{decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35881\n",
      "['Hello', ' World', ' wouldn', \"'t\", \"'VE\"]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "# gpt2 tokenizer\n",
    "gpt2 = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", re.IGNORECASE)\n",
    "\n",
    "this = re.findall(gpt2, text)\n",
    "print(len(this))\n",
    "print(re.findall(gpt2, \"Hello World wouldn't'VE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100276\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.max_token_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endofprompt|>\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode([100276]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
