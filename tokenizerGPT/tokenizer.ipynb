{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "# Wikipedia article setup\n",
    "webpage = \"Python (programming language)\" # wikipedia article title\n",
    "\n",
    "def fetchWiki(term, wiki_lang=\"en\"):\n",
    "    wiki = wikipediaapi.Wikipedia(user_agent=\"wikiScraper/1.0 (avengerdoge999@gmail.com)\", language=wiki_lang)\n",
    "    page = wiki.page(term)\n",
    "    if page.exists():\n",
    "        return page.summary, page.text\n",
    "    else:\n",
    "        print(f\"{term} could not be found\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "with open('input.txt', 'r') as f, open(\"output.txt\", \"w\") as w:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        summary, text = fetchWiki(line)\n",
    "        w.write(f\"{summary}\\n\\n{text}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Theory: Iterate through string some times and get pairs of characters, the most common pair is replaced with some symbol\n",
    "\n",
    "aaabdaaabac -> ZabdZabac -> ZYdZYac -> XdXac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192808\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 merging (101, 32) to 256\n",
      "30 merging (281, 32) to 286\n",
      "60 merging (115, 117) to 316\n",
      "90 merging (274, 32) to 346\n",
      "120 merging (363, 32) to 376\n",
      "150 merging (110, 111) to 406\n",
      "180 merging (299, 115) to 436\n",
      "210 merging (111, 98) to 466\n",
      "240 merging (109, 307) to 496\n",
      "270 merging (293, 265) to 526\n",
      "300 merging (275, 382) to 556\n",
      "330 merging (374, 49) to 586\n",
      "360 merging (440, 493) to 616\n",
      "390 merging (119, 581) to 646\n",
      "420 merging (273, 471) to 676\n",
      "450 merging (273, 108) to 706\n",
      "480 merging (265, 276) to 736\n",
      "510 merging (403, 32) to 766\n",
      "540 merging (117, 279) to 796\n",
      "570 merging (326, 378) to 826\n",
      "600 merging (10, 448) to 856\n",
      "630 merging (437, 289) to 886\n",
      "660 merging (257, 287) to 916\n",
      "690 merging (334, 104) to 946\n",
      "720 merging (315, 364) to 976\n",
      "66475\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "vocab_size = 1000\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "merge_interval = 30\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    if (i % merge_interval == 0):\n",
    "        print(f\"{i} merging {pair} to {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.90\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens) / len(ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        if len(stats) == 0:\n",
    "            break\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'K'\n",
      "'im'\n"
     ]
    }
   ],
   "source": [
    "for i in encode(\"Kim\"):\n",
    "    print(f\"'{decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35881\n",
      "['Hello', ' World', ' wouldn', \"'t\", \"'VE\"]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "# gpt2 tokenizer\n",
    "gpt2 = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", re.IGNORECASE)\n",
    "\n",
    "this = re.findall(gpt2, text)\n",
    "print(len(this))\n",
    "print(re.findall(gpt2, \"Hello World wouldn't'VE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100276\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.max_token_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endofprompt|>\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode([100276]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Applications/GPT/.venv/lib/python3.12/site-packages/tiktoken/core.py:117\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[38;5;241m.\u001b[39msearch(text):\n\u001b[0;32m--> 117\u001b[0m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_core_bpe\u001b[38;5;241m.\u001b[39mencode(text, allowed_special)\n",
      "File \u001b[0;32m~/Applications/GPT/.venv/lib/python3.12/site-packages/tiktoken/core.py:398\u001b[0m, in \u001b[0;36mraise_disallowed_special_token\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as a special token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "print(enc.encode('<|endoftext|>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
